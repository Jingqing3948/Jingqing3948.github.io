<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta name="description" content="吴恩达机器学习课程学习笔记" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />

      <meta name="google-site-verification" content="YwYbA1LbL6DK2afClSAZcmUaT2QiD4rluljLHHU4120" />
      
      <title>技术学习：吴恩达机器学习 |  灰海宽松的博客</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      /> 
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>
      <!-- <link
        rel="stylesheet"
        href="https://cdn.staticfile.org/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.staticfile.org/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script> -->

      <!-- mermaid -->
      
      <script src="https://cdn.staticfile.org/mermaid/8.14.0/mermaid.min.js"></script>
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      <canvas class="fireworks"></canvas>
      <style>
        .fireworks {
          position: fixed;
          left: 0;
          top: 0;
          z-index: 99999;
          pointer-events: none;
        }
      </style>
      
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-技术学习：吴恩达机器学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  技术学习：吴恩达机器学习
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/05/31/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2025-05-30T23:00:00.000Z" itemprop="datePublished">2025-05-31</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%AE%97%E9%81%93%E6%B1%82%E7%B4%A2%EF%BC%88%E8%AF%BE%E5%A4%96IT%E6%8A%80%E8%83%BD%E5%AD%A6%E4%B9%A0%EF%BC%89/">算道求索（课外IT技能学习）</a> / <a class="article-category-link" href="/categories/%E7%AE%97%E9%81%93%E6%B1%82%E7%B4%A2%EF%BC%88%E8%AF%BE%E5%A4%96IT%E6%8A%80%E8%83%BD%E5%AD%A6%E4%B9%A0%EF%BC%89/Lecture/">Lecture</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">13.5k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">50 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="前言">前言</h2>
<p>学习自 Coursera 的吴恩达老师机器学习系列。<a target="_blank" rel="noopener" href="https://www.coursera.org/specializations/machine-learning-introduction">https://www.coursera.org/specializations/machine-learning-introduction</a></p>
<h2 id="机器学习定义">机器学习定义</h2>
<p>计算机不经过明确编程的情况下进行学习的研究领域。比如相比直接编程告诉计算机怎么下棋更容易取胜，让计算机自己经过多次尝试总结形成一个具有较高胜率的下棋模型，每一步自行预测下一步棋怎么走让最终胜率更大。</p>
<blockquote>
<p>[!NOTE]</p>
<p>在 KCL 机器学习笔记中我们学习过：推理（Inference）和学习（Learning）的区别。推理是已知模型和对应的概率（现实生活中很少），而学习是不知道对应的模型和概率需要统计数据训练得到一个近似值。比如投硬币，我们统计了 100 次投硬币概率，30 次正面 70 次反面，那么学习结果就是 0.3 概率正面。</p>
</blockquote>
<h3 id="监督学习">监督学习</h3>
<p>给定输入 x 和有标签标注的输出 y，如对于天气预报机器学习案例：（昨天有鱼鳞状云，今天下雨）这样一条数据，昨天的云是输入数据 x，今天下雨是输出 y（知道确切的结果），让计算机由输入到输出的映射训练模型。再比如：输入是一栋房子的基本信息（几室几厅，面积……）输出是其价格。</p>
<p>监督学习主要分为两种：</p>
<ul>
<li>回归问题（Regression）：从无限多的数字中预测一个数字，如房价推测。</li>
<li>分类问题（Classification）：如判断这张图片里的动物是猫还是狗，明天是否下雨，预测结果有限可确定。比如可能 0 代表照片里动物是猫，1 代表是狗，2 代表是老鼠……</li>
</ul>
<h3 id="无监督学习">无监督学习</h3>
<p>没有对应的有标注的输出 y。比如找到一些潜在的分类（把现有用户分为几种，分类之前我们并不知道有哪些最终的用户类，是计算机帮我们划分的）。这叫做聚类（Clustering）。除此之外的应用还有异常情况检测，新闻关键词组合提取等。</p>
<h2 id="监督学习-2">监督学习</h2>
<h3 id="线性回归模型">线性回归模型</h3>
<blockquote>
<p>[!NOTE]</p>
<p>模型，就是机器学习算法所需要的最终产物。通过数据和算法得到。比如 DeepSeek 模型，GPT 模型。使用模型很简单其实，我们不需要关心模型内部原理，就像一个黑盒子，我们只要输入数据进去得到输出数据就好。不过这门课程我们主要学习“训练模型的过程”，了解学习原理。</p>
</blockquote>
<p>线性回归模型是一种非常基础的回归模型。比如根据输入数据（房子面积）预测输出数据（房价）公式：y = 20000x+100000（我瞎写的）这就是一个简单的线性回归模型。</p>
<h4 id="简化版训练过程">简化版训练过程</h4>
<p>线性回归模型训练过程（无敌简化版）：</p>
<ol>
<li>首先我们有一个训练数据集，里面只有两条数据（0,100000）和（2,140000）。训练数据集数据总数一般用 m 表示。</li>
<li>线性回归模型我们设置成 <code>y=wx+b</code>，w 一般用于表示模型参数，b 表示线性回归模型后面加的常量。</li>
<li>带入数据，得到 w = 20000，b = 100000. 完成训练。得到的模型用 <code>f_wb</code> 表示。</li>
</ol>
<h4 id="代价函数">代价函数</h4>
<p>不过，一般真实数据都不是那么有规律的，我们的模型并非可以完美拟合（fit）所有数据点。比如下面这个情况。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021403294.png" alt=" "  />
<p>这些红叉数据点就不在一条直线上，我们怎么可能找到一个完美拟合的线性回归模型经过他们全部呢，做不到。只能尽可能找到一个线性回归模型让这些点离我们模型的偏差尽量小一些。</p>
<p>这就引入了一个机器学习重要概念：代价（cost）也就是我们的模型预测值和真实值的偏差。对于单个数据点偏差一般用损失（loss）表示，比如我预测这栋有 100m^2 的房子房价是 100 万元，实际价格是 110 万元。而代价一般是形容模型整体，所有预测数据和所有训练数据的偏差。</p>
<p>一种成本函数计算公式（成本函数有很多很多选择，就像模型一样，我们也可以不选线性回归模型学习问题）：<br>
$$<br>
J(w, b)=\frac{1}{2m}\sum^{m}_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})^2<br>
$$<br>
$y^{(i)}$ 是第 i 个训练数据的输出（110 万真实房价），$f_{w,b}(x^{(i)})$ 是将第 i 个训练数据输入带入我们的模型得到的预测输出（100 万预测房价）。我们的模型预测目标就是找到合适的模型参数 w，b 让代价函数最小化。</p>
<blockquote>
<p>[!NOTE]</p>
<p>这里前面的参数为什么是 1/2m，因为后续我们要求导来找到让代价函数下降最快的方式，求导的时候就会把 2 约掉，计算更简洁。</p>
</blockquote>
<p>下图是一个代价函数可视化案例：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021412818.png" alt=" " style="zoom: 33%;" />
<p>对于这个例子我们要找到的代价函数最小值点就是中间那个凹进去的地方对吧。</p>
<p>另一种绘制方式是这样的等高线图：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021413456.png" alt=" " style="zoom:50%;" />
<h4 id="梯度下降">梯度下降</h4>
<p>根据当前训练数据，通过求偏导更新 w，b 让代价函数下降最快（Gradient Descent）。</p>
<p>计算公式：<br>
$$<br>
w = w-\alpha \frac{d}{dw}J(w, b)=\frac{1}{m}\sum^{m}_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})x^{(i)}\\<br>
b = b-\alpha \frac{d}{db}J(w, b)=\frac{1}{m}\sum^{m}_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})<br>
$$<br>
α 是学习率，α 越大每一轮学习的时候我们根据上一轮的代价函数所做的调整就越大。</p>
<p>α 太小的话，会导致需要很多轮才能找到一个比较靠近极小值的模型：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021419694.png" alt=" " style="zoom:50%;" />
<p>α 太大有可能总是“迈步迈过头”到达不了极小值点。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021419405.png" alt=" " style="zoom:50%;" />
<p>另外，一个学习问题可能会有多个局部极小值点。选择不恰当大小的学习率也会导致无法到达最小值。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021424024.png" alt=" " style="zoom: 25%;" />
<p>下图是一个梯度下降参数可视化：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021425684.png" alt=" " style="zoom:33%;" />
<blockquote>
<p>[!NOTE]</p>
<p>另外，每轮学习的时候我们可能会只选取部分训练数据进行迭代更新或者全部数据。如果每次都用全部数据计算代价函数，叫做批梯度下降（Batch Gradient Descent BGD）.</p>
<p>卷积神经网络（CNN）训练不会每次都看整个图像数据，每次只会取出一部分。这样可以加快计算速度而且更不容易出现过拟合。</p>
</blockquote>
<blockquote>
<p>[!TIP]</p>
<p>老师提出的一种寻找学习率的方式：</p>
<p>如 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, …</p>
<p>此外，还可以使用 Adam 算法寻找合适的学习率。原理上和自己试应该差不多，从小到大试，loss 快速下降时停止增加学习率。</p>
</blockquote>
<h4 id="多维度特征">多维度特征</h4>
<p>特征（features）指的就是输入数据多维度的参数，比如：如果只根据房屋面积计算房屋价格，特征就只有一个就是面积。如果根据房屋面积，几室几厅，楼层数，房屋年龄等作为输入数据，就是多特征学习。</p>
<p>我们用 n 表示特征总数，那么输入数据其实就是一个 m*n 的矩阵，m 表示有 m 条数据，n 表示每条数据里面有 n 个特征。$x_j^{(i)}$ 表示第 i 条数据的第 j 个属性。</p>
<p>同时模型参数也会改变，$w=[w_1, w_2, … w_n]$ 和 $x^{(i)}$ 进行向量乘法得到一个标量数据，再和标量 b 求和得到预测结果。<br>
$$<br>
f_{w, b}(\mathbf{x}) = \mathbf{w}\cdot \mathbf{x} +b = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b<br>
$$<br>
在 python 代码实现中，不需要手动循环相乘求和，可以使用 <code>numpy</code> 库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([<span class="number">1</span>, <span class="number">2.5</span>, -<span class="number">3.3</span>])</span><br><span class="line">b = <span class="number">4</span></span><br><span class="line">x = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])</span><br><span class="line"></span><br><span class="line">f = np.dot(w,x)+b</span><br></pre></td></tr></table></figure>
<p>而且计算机底层会使用带有矢量化的计算机硬件实现，并行加速计算。</p>
<p>同长度向量可以直接做减法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = w - <span class="number">0.1</span> * b</span><br></pre></td></tr></table></figure>
<blockquote>
<p>[!NOTE]</p>
<p>如果特征量太大，使用正态方程法可以快速求解 w，b。但是这种方法几乎只适用于线性回归学习方法。</p>
</blockquote>
<blockquote>
<p>[!NOTE]</p>
<p>有的时候可能还需要我们自行设计特征，比如根据房子长宽推导房子面积，如果使用二维度线性回归模型效果就不会太好，但是如果将长*宽作为一个新维度特征输入的一维线性回归模型训练就会好一些。</p>
</blockquote>
<h4 id="特征缩放">特征缩放</h4>
<p>多维度特征尽可能采取范围近似的值，让彼此参数差距不大，否则可能导致梯度下降缓慢。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021457653.png" alt=" " style="zoom: 33%;" />
<p>简单的特征缩放方法就是（每个数据-平均值）/（最大值-最小值）。但是分母可能是 0，而且对异常值敏感。</p>
<p>z-score normalization 方法是（数据-平均值）/标准差 σ。转换为均值 0，方差 1 的标准正态分布区间。</p>
<h4 id="梯度下降收敛">梯度下降收敛</h4>
<p>当每轮迭代的梯度下降小于等于收敛值 ε（如 0.001）时，可以称之为收敛。这个时候就可以不用再学习了，再学习的成本与对模型精确度的提升相比不那么合适了。</p>
<h4 id="扩展到多项式回归">扩展到多项式回归</h4>
<p>$$<br>
f_{\vec{w}, b}({x}) =<br>
{w_1} {x} +<br>
{w_2} {x^2} +<br>
{w_3}{x^3} +<br>
{b}<br>
$$</p>
<h3 id="分类">分类</h3>
<p>比如：预测一个邮件是否为垃圾邮件，0 false 1 true。</p>
<p>预测方法：比如线性回归模型得到的结果是 0 到 1 范围内，我们将 0-0.5 视作 0,0.5-1 视作 1.</p>
<p>不过相比线性回归模型，下面这个回归模型更适合分类问题：</p>
<h4 id="逻辑回归">逻辑回归</h4>
<ol>
<li>首先还是得到一个线性回归模型 $z=wx+b$。</li>
<li>然后不直接把这个模型作为预测输出，而是将其传入 sigmoid 函数：$g(z)=\frac{1}{1+e^{-z}}$</li>
</ol>
<p>为什么用这个函数？因为其范围 0-1，符合我们的分类需求，而且在 0.5 附近斜率比较大，所以更难预测到 0.5 的不确定概率情况。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506021532641.png" alt=" " style="zoom: 67%;" />
<h4 id="逻辑回归的损失函数">逻辑回归的损失函数</h4>
<p>不能再使用之前的平方代价函数了，因为转换得到的代价函数并不是一个凸函数（convex，比如 y = x^2 这样的），而可能是一个波浪线状的有多个极小值点的丑陋函数。</p>
<p>所以要使用下面这个损失函数。<br>
$$<br>
L\left( f_{\vec{w}, b}(\vec{x}^{(i)}),\ y^{(i)} \right) =<br>
\begin{cases}<br>
-\log\left( f_{\vec{w}, b}(\vec{x}^{(i)}) \right), &amp; \text{if } y^{(i)} = 1 \\<br>
-\log\left( 1 - f_{\vec{w}, b}(\vec{x}^{(i)}) \right), &amp; \text{if } y^{(i)} = 0<br>
\end{cases}\\<br>
L\left( f_{\vec{w}, b}(\vec{x}^{(i)}),\ y^{(i)} \right) =</p>
<ul>
<li>y^{(i)} \log\left( f_{\vec{w}, b}(\vec{x}^{(i)}) \right)</li>
<li>(1 - y^{(i)}) \log\left( 1 - f_{\vec{w}, b}(\vec{x}^{(i)}) \right)<br>
$$<br>
逻辑回归损失函数图像如下：</li>
</ul>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022137019.png" alt=" " style="zoom: 50%;" />
<p>y = 1 预测值 = 0 的时候导数非常大，y = 0 预测值 = 1 的时候导数非常大，促使预测值向另一端靠近。</p>
<p>相应的代价函数公式：<br>
$$<br>
\begin{aligned}<br>
J(\vec{w}, b) &amp;= \frac{1}{m} \sum_{i = 1}^{m}<br>
L\left( f_{\vec{w}, b}(\vec{x}^{(i)}),\ y^{(i)} \right)\\<br>
&amp;= -\frac{1}{m} \sum_{i = 1}^{m} \left[<br>
y^{(i)} \log\left( f_{\vec{w}, b}(\vec{x}^{(i)}) \right)</p>
<ul>
<li>(1 - y^{(i)}) \log\left( 1 - f_{\vec{w}, b}(\vec{x}^{(i)}) \right)<br>
\right]<br>
\end{aligned}<br>
$$<br>
至于梯度下降，公式和之前一样。不过别忘记现在 f 函数的定义不一样了。<br>
$$<br>
w = w-\alpha \frac{d}{dw}J(w, b)=\frac{1}{m}\sum^{m}_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})x^{(i)}\\<br>
b = b-\alpha \frac{d}{db}J(w, b)=\frac{1}{m}\sum^{m}_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})<br>
$$</li>
</ul>
<h4 id="过拟合及解决方法">过拟合及解决方法</h4>
<p>如下图，左边的模型欠拟合（underfit）因为模型复杂度不够；而右边的模型过拟合（overfit）因为参数太多模型过于复杂了，导致模型能非常好地拟合训练数据，但是对于训练数据之外的数据预测效果不好。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022142124.png" alt=" " style="zoom:67%;" />
<p>过拟合的解决方式：</p>
<ol>
<li>增加训练数据量；</li>
<li>只选择部分对输出数据影响更大的特征作为训练模型的特征，减小模型复杂度；</li>
<li>模型正交化（Regularization），将模型复杂度也列入代价函数公式中，这样模型过于复杂的时候虽然可以很好地拟合训练数据，代价函数仍然很大。比如加上 $\frac{\lambda}{2m}\sum^{n}_{j=1}w_j^2$ ，λ 是正交化参数。</li>
</ol>
<p>$$<br>
J(w, b)=\frac{1}{2m}\sum^{m}_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum^{n}_{j = 1}w_j^2\\<br>
w_j = w_j - \alpha<br>
\frac{\lambda}{m} w_j - \alpha<br>
\frac{1}{m} \sum_{i = 1}^{m}<br>
\left( f_{w, b}(\vec{x}^{(i)}) - y^{(i)} \right) x_j^{(i)}\\<br>
b =\frac{1}{m}\sum^{m}_{i = 1}(f_{w, b}(x^{(i)})-y^{(i)})<br>
$$</p>
<p>对梯度下降的影响其实就是 w 更新的时候前面一项由 wj 变成 (1-αλ/m)wj 了。这个东西 &lt; 1 ，作用是每次迭代都让 wj 尽可能小一些。</p>
<h3 id="神经网络">神经网络</h3>
<p>模拟人脑，分为多层对输入数据进行处理，每层有多个神经元，处理完输出给下一层，最后汇总输出。</p>
<p>在更大数据量或者更复杂的问题上表现更好。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022157643.png" alt=" " style="zoom: 33%;" />
<p>比如人脸识别学习案例：如下图，第一层神经网络可能着重识别边缘部分，第二层神经网络可能着重识别人脸的某一部分比如眼睛鼻子嘴巴，第三层则是匹配对应的整张人脸。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022158185.png" alt=" " style="zoom: 33%;" />
<p>输入数据被称为输入层（input layer），一般不计入总层数。中间的层都叫隐藏层（hidden layer），最后一层叫输出层（output layer）计入总层数，比如上面这个例子是一个 4 层神经网络。</p>
<p>每个神经元的输入 -&gt; 输出处理函数叫做激活函数（activation function），比如我们可能第一层选用线性回归函数，第二层选用 sigmoid 函数……</p>
<p>比如假设第三层是 sigmoid 激活函数，第三层有三个神经元，那么第三层就会输出三个特征，第三层输出向量计算公式为：<br>
$$<br>
a_1^{[3]}= g(\vec(w)_1^{[3]}\cdot \vec{a}^{[2]}+b_1^{[3]})\\<br>
a_2^{[3]}= g(\vec(w)_2^{[3]}\cdot \vec{a}^{[2]}+b_2^{[3]})\\<br>
a_3^{[3]}= g(\vec(w)_3^{[3]}\cdot \vec{a}^{[2]}+b_3^{[3]})\\<br>
$$<br>
这里会涉及到一些矩阵乘法的知识。</p>
<p>例学习问题：输入数据是烘焙咖啡豆温度和时间，输出数据是咖啡豆烘焙结果（刚好或糊了）。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022213779.png" alt=" " style="zoom:67%;" />
<p>比如在 TensorFlow 中配置模型的步骤如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">model = Sequential(</span><br><span class="line">    [</span><br><span class="line">        tf.keras.Input(shape=(<span class="number">2</span>,)),</span><br><span class="line">        Dense(<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer1&#x27;</span>),</span><br><span class="line">        Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name = <span class="string">&#x27;layer2&#x27;</span>)</span><br><span class="line">     ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看每层参数</span></span><br><span class="line">W1, b1 = model.get_layer(<span class="string">&quot;layer1&quot;</span>).get_weights()</span><br><span class="line">W2, b2 = model.get_layer(<span class="string">&quot;layer2&quot;</span>).get_weights()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置损失函数</span></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss = tf.keras.losses.BinaryCrossentropy(),</span><br><span class="line">    optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动训练</span></span><br><span class="line">model.fit(</span><br><span class="line">    Xt,Yt,            </span><br><span class="line">    epochs=<span class="number">10</span>, <span class="comment"># 用输入数据重复训练10次</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">predictions = model.predict(X_testn)</span><br></pre></td></tr></table></figure>
<p>使用起来非常简单，调库有啥难的？不过重点还是在于原理理解。</p>
<p>这个例子中，输入数据是 2 维特征的，第一层隐藏层是 3 维的，第二层输出层是 1 维的，我们最终会得到一个预测结果值。</p>
<p>对于第一层的模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">W = np.array([</span><br><span class="line">    [<span class="number">1</span>, -<span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">    [<span class="number">2</span>,  <span class="number">4</span>, -<span class="number">6</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">b = np.array([-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])     <span class="comment"># 偏置项，形状为 (3,)</span></span><br><span class="line">a_in = np.array([-<span class="number">2</span>, <span class="number">4</span>])     <span class="comment"># 输入向量（也记作 a^[0] 或 x），形状为 (2,)</span></span><br></pre></td></tr></table></figure>
<p>这样计算完得到的是一个 1*3 形状的矩阵，供第二层输入。</p>
<p>在 python 中利用 numpy 做向量矩阵快速乘法的实现公式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">g</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="comment"># 示例激活函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))  <span class="comment"># Sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dense</span>(<span class="params">A_in, W, B</span>):</span><br><span class="line">    Z = np.matmul(A_in, W) + B</span><br><span class="line">    A_out = g(Z)</span><br><span class="line">    <span class="keyword">return</span> A_out</span><br></pre></td></tr></table></figure>
<h4 id="ReLU-激活函数">ReLU 激活函数</h4>
<p>如下图，ReLU 函数公式很简单，输入 &lt;0 时输出=0，输入 &gt; 0 时输出和输入成正比。</p>
<p><img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022219074.png" alt=" "></p>
<p>sigmoid 函数只有在 0 附近的输入值才能取得比较好的梯度结果，如果输入数据太大了，那么梯度下降特征不明显，可能会造成梯度消失的情况，而且计算复杂；</p>
<p>以及，ReLU 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</p>
<p>最常见的四种激活函数就是：线性激活函数，sigmoid，ReLU，softmax 后面介绍。</p>
<h4 id="激活函数的选择">激活函数的选择</h4>
<p>二元分类问题推荐使用 sigmoid，因为最终推导结果介于 0~1 之间。</p>
<p>回归问题建议使用线性回归激活函数。</p>
<p>回归问题且输出 &gt; 0 推荐使用 ReLU。</p>
<p>比如一个二元神经网络问题，采用的激活函数可能是：前两层是 ReLU，最后一层是 Sigmoid。</p>
<p>此外，隐藏层尽量不要使用线性函数（ReLU 可以用），因为根据公式推导，隐藏层放多个线性回归最后得到的输出也是一个线性回归函数，使得这个神经网络解决不了更加复杂的问题。</p>
<h4 id="多分类问题">多分类问题</h4>
<p>比如手写数字识别，我们预测的结果介于 0~9 之间。</p>
<p>softmax 常用于解决多分类问题，可以得到多个特征的输出结果。比如对于手写数字识别问题，可能得到的结果是：预测是数字 0 的概率为 0.09,1 的概率是 0.11,2 的概率是……最后选择一个最有可能的结果。</p>
<p>算法公式：<br>
$$<br>
z_j = \vec{w}_j \cdot \vec{x} + b_j, \quad j = 1, \dots, N\\<br>
a_j = \frac{e^{z_j}}{\sum\limits_{k = 1}^{N} e^{z_k}} = \mathbb{P}(y = j \mid \vec{x})<br>
$$<br>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022229410.png" alt=" " style="zoom: 33%;" /></p>
<p>所有预测概率加起来 = 1.</p>
<p>损失函数：<br>
$$<br>
\text{loss}(a_1, \dots, a_N, y) =<br>
\begin{cases}<br>
-\log a_1 &amp; \text{if } y = 1 \\<br>
-\log a_2 &amp; \text{if } y = 2 \\<br>
\quad \vdots &amp; \\<br>
-\log a_N &amp; \text{if } y = N \\<br>
\end{cases}<br>
$$</p>
<p>损失函数只有对应真实值的预测概率 = 1 时 = 0，否则预测正确结果的概率越小，损失值越大。</p>
<p>在多分类预测问题中，通常使用 softmax 来作为输出层的激活函数。</p>
<h4 id="改进-softmax-函数">改进 softmax 函数</h4>
<p>根据 python 的一些精度特性，如果不是先计算出概率 a 再带入损失函数，而是直接把 z 带入损失函数计算公式，精度更高。</p>
<p>不要：<br>
$$<br>
\text{Loss} = L(\vec{a}, y) =<br>
\begin{cases}<br>
-\log a_1 &amp; \text{if } y = 1 \\<br>
\vdots \\<br>
-\log a_{10} &amp; \text{if } y = 10 \\<br>
\end{cases}<br>
$$<br>
而是：<br>
$$<br>
L(\vec{z}, y) =<br>
\begin{cases}<br>
-\log \left( \frac{e^{z_1}}{e^{z_1} + \cdots + e^{z_{10}}} \right) &amp; \text{if } y = 1 \\<br>
\vdots \\<br>
-\log \left( \frac{e^{z_{10}}}{e^{z_1} + \cdots + e^{z_{10}}} \right) &amp; \text{if } y = 10 \\<br>
\end{cases}<br>
$$</p>
<p>在 TensorFlow 中的实现方法应该是：最后一层不再声明为 softmax 层，而是 linear 层，取而代之的是在模型参数设置的时候声明计算逻辑损失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">preferred_model = Sequential(</span><br><span class="line">    [ </span><br><span class="line">        tf.keras.Input(shape=(<span class="number">2</span>,)),</span><br><span class="line">        Dense(<span class="number">25</span>, activation = <span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        Dense(<span class="number">15</span>, activation = <span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        Dense(<span class="number">4</span>, activation = <span class="string">&#x27;linear&#x27;</span>)   <span class="comment">#&lt;-- Note</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">preferred_model.<span class="built_in">compile</span>(</span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),  <span class="comment">#&lt;-- Note</span></span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(<span class="number">0.001</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="多标签分类问题">多标签分类问题</h4>
<p>比如：判断下面这张图片里面有没有汽车？有没有行人？有没有树？</p>
<p>和多分类问题的区别在于，多分类的结果是互斥的，比如预测出数字应该是 5 就不会是 7 了。</p>
<p>而多标签分类问题的多个结果可以共存，比如既有树，也有人。</p>
<p>多标签分类也会得到多个维度的特征，不过类似 sigmoid 单维度处理方式，每个维度的预测值在 0-1 之间，如果 &gt; 0.5 视作 true。比如对有汽车的概率预测为 0.6，有人的概率预测为 0.7，那么就判断这张图片里有人有车。</p>
<h4 id="反向传播">反向传播</h4>
<p>说实话有点太难描述，但是这部分十分重要。读者如果感兴趣一定要去学习一下，其实不难，就是求导反推。</p>
<p>可以快速推导出 w 的变化对最终代价函数 j 的影响。</p>
<h3 id="机器学习应用问题">机器学习应用问题</h3>
<h4 id="解决误差大问题">解决误差大问题</h4>
<p>如果模型误差很大，下一步怎么做？</p>
<ol>
<li>获取更多数据。</li>
<li>减少或者增加特征。</li>
<li>增加多项式参数。</li>
<li>调整正则化参数 λ。</li>
</ol>
<h4 id="评估模型">评估模型</h4>
<p>如何评估模型性能？通常我们会将输入数据拿出 30%作为测试集，其余 70%作为训练集。先用训练集训练，然后用测试集和训练集评估误差。</p>
<p><strong>注意：测试集和训练集评估误差的时候不要加上正交参数，这个时候最小化参数不是我们想要的结果，我们是在评估模型性能。</strong></p>
<p>如果模型对训练数据表现良好，对测试数据表现差，说明过拟合。</p>
<p>但其实，<strong>测试集仅仅是我们对模型误差的一种“乐观估计”</strong>。因为测试集通常来自与训练集相似的分布，而且在整个建模过程中，它是提前准备好的。如果我们在调参过程中频繁查看测试集的表现，实际上已经“间接使用”了测试集的信息。这会导致模型过拟合到测试集本身，最终得出的泛化能力评估并不可靠。</p>
<p>取而代之的方案是 60%数据作为训练集，20%数据作为交叉验证集（Cross Validation），20%数据作为测试集。对于不同模型（如一次线性，二次线性，三次线性……）我们统一用训练集训练，然后用 <strong>交叉验证集</strong> 计算代价，注意这里也不要考虑参数正交化项。然后选出交叉验证误差最小的模型。最终我们用测试集评估模型泛化性能。以多项式拟合为例：我们可以用训练集分别训练一次线性、二次多项式、三次多项式等模型，然后在验证集上评估它们的预测误差（此时评估用的 cost 函数不需要包含正则化项），选出在验证集上表现最好的模型。最后，再用测试集来评估这个最终模型在未见数据上的表现。</p>
<p>比如下面这个学习案例：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506022345189.png" alt=" " style="zoom:67%;" />
<p>degree = 4 的时候训练集和交叉验证集误差都最小，5 变化不大，6 开始回升了。所以最佳选择 4.</p>
<p>分类问题和神经网络学习问题同样。</p>
<h4 id="Bias-and-Variance">Bias and Variance</h4>
<p>underfit 模型的 bias 很大，overfit 模型的 variance 很大。</p>
<p>下图是随着多项式回归模型项数增加，训练代价和交叉验证代价的变化，在中间训练和交叉验证代价都比较小的模型才比较合适。</p>
<img src="C:\Users\123123\AppData\Roaming\Typora\typora-user-images\image-20250603193106733.png" alt="" style="zoom:50%;" />
<p>对于参数 λ 的选择也是类似。正交化参数应该多大比较好？我们先设置不同大小的正交化参数，对训练集进行训练得到模型参数 w，b，然后带入交叉验证计算损失，绘图如下，λ 让两个损失都比较小的值更为合适。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506031941804.png" alt=" " style="zoom:50%;" />
<p>具体 bias and variance 的判断方法：我们使用三个参数进行判断，基准表现（baseline performance）如人类自己对语音的识别准度，训练误差，交叉验证误差。</p>
<p>基准表现和训练误差差的较大：偏差（bias）很大。</p>
<p>训练误差和交叉验证误差差距较大：方差（variance）很大。</p>
<p>训练数据 m 增加，交叉验证损失和训练集损失如下：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506031950327.png" alt=" " style="zoom:50%;" />
<p>如果两者最后汇总的误差仍然很大，说明 bias 很大。</p>
<p>如果两者最后没有汇总，差距比较大，说明 variance 很大。方差大的情况增加训练数据确实有助于减少交叉验证损失。</p>
<p>构建机器学习应用的时候，比如有 1000 个训练数据，可以先拿出 100 个用于训练集和 CV 集，然后绘制学习曲线判断 bias 和 variance。</p>
<h4 id="解决正交化后误差仍然大问题">解决正交化后误差仍然大问题</h4>
<ul>
<li>更多训练数据：减小方差</li>
<li>减少特征：减小方差</li>
<li>增加特征：减小偏差</li>
<li>增加多项式项数：减小偏差</li>
<li>减少 lambda：减小偏差</li>
<li>增加 lambda：减小方差</li>
</ul>
<p>方差和偏差很好理解，但是可能需要很长时间才能深入把控。</p>
<h4 id="神经网络解决偏差方差过大问题">神经网络解决偏差方差过大问题</h4>
<p>对于神经网络，有一种思想是：只要网络够大，训练数据够多，就能解决偏差和方差问题。</p>
<pre class="mermaid">
flowchart LR
    Start([开始：训练神经网络模型])

    Start --> CheckTrainError{训练误差 J_train 高吗？}

    CheckTrainError -- 是 --> HighBias[高偏差问题\n→ 增大网络规模（更多层/神经元）]
    HighBias --> Retrain1[重新训练模型]
    Retrain1 --> CheckTrainError

    CheckTrainError -- 否 --> CheckValError{验证误差 J_cv 明显更高吗？}

    CheckValError -- 否 --> Done([✅ 训练完成！模型具备良好的泛化能力])
    CheckValError -- 是 --> HighVar[高方差问题\n→ 收集更多数据或添加正则项]
    HighVar --> Retrain2[重新训练模型]
    Retrain2 --> CheckTrainError</pre>
<p>至于过拟合问题，只要神经网络层中有合适的正交化参数就能解决。</p>
<h4 id="机器学习开发过程">机器学习开发过程</h4>
<ol>
<li>选择架构：如模型，数据集等。</li>
<li>训练模型。</li>
<li>诊断（解决方差，标准差等问题）。</li>
<li>重复如上步骤。</li>
</ol>
<h5 id="误差分析">误差分析</h5>
<p>对于错误分类的例子进行人工分析（如果太大了就按比例抽取），比如错误分类的非垃圾邮件。</p>
<h5 id="添加更多数据的技巧">添加更多数据的技巧</h5>
<p>数据增强：比如扭曲图像数据。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506032129157.png" alt=" " style="zoom:50%;" />
<p>给音频数据添加噪声。</p>
<p>数据合成：一般用于计算机视觉干扰。利用计算机模拟或者算法生成的带有标注的信息。比如自己利用电脑字体库合成带有字的图片的数据供 OCR 进行训练。</p>
<h5 id="迁移学习">迁移学习</h5>
<p>用不同任务的数据来解决当前的任务。</p>
<p>比如动物识别迁移到手写数字图片识别。</p>
<p>因为两个网络本质都是图像分类，所以在隐藏层的大部分工作都是相似或者一样的，所以将其中一个训练好的网络的隐藏层拿过来用，对输出层重新训练就可以实现新的功能（相当于螺丝刀换刀头）。</p>
<h4 id="偏置数据集">偏置数据集</h4>
<p><img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506032206512.png" alt=" "></p>
<p>精度（Precision）：预测为 true 的输出数量中真实值为 true 的样本数占比（True Positive / Positive，上边第一行）</p>
<p>召回率（Recall）：真的阳性的样本中被预测到了的占比（True Positive / Actual Positive，左边第一个纵列）</p>
<p>高精度意味着准确（检测结果为阳性的病人很大概率是真阳性）；高召回率意味着阳性案例很大概率会被检测到，很少漏掉。</p>
<p>提高 True 案例的判断阈值（比如：预测结果大于 0.7 的才预测为 positive，而不是 0.5），会提高精度 降低召回率；反之，降低判断阈值会降低精度，提高召回率。</p>
<p>我们需要手动调整判断阈值来找到权衡精度和召回率的点。一种常见的权衡方法是谐波均值（Harmonic Mean F1 Score），计算公式如下：<br>
$$<br>
F1\ Score\ =\frac{2PR}{P+R}<br>
$$<br>
然后我们选择谐波均值更高的模型。</p>
<h3 id="决策树">决策树</h3>
<p>根据数据训练出一个多分类组成的决策树模型。</p>
<p>比如下面是一个可能的决策树模型（判断动物是不是猫）：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506032337220.png" alt=" " style="zoom: 50%;" />
<h4 id="纯度">纯度</h4>
<p>如何选择决策树的分支特征？一个很重要的因素是左右子树的纯度（purity）。比如我们挑了一个特征：是否为黑色。那么就导致左右子树都有猫，因为猫有黑的有非黑的。这就是一个不纯的分支。每一个分支点尽量让猫全在一个子树里面。</p>
<h4 id="停止分支条件">停止分支条件</h4>
<p>什么时候停止分支？</p>
<ul>
<li>当一个节点 100%是同一种类时（没必要分了）。</li>
<li>再分树就超过最大深度了。</li>
<li>分支让纯度的提升太小（小于一定阈值）。</li>
<li>分支分出去的一部分子树节点太少（小于一定阈值）。</li>
</ul>
<p>纯度用熵来衡量，熵的图表对应如下（p 是一个集合里面目标分类的个数，比如：这个集合里面猫占比）。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506042300977.png" alt=" " style="zoom:50%;" />
<p>所以从图中可以看出 p = 0.5 的时候熵最高 = 1.</p>
<p>熵的曲线近似计算公式如下（不完全符合，比如下面这个公式 = 0 的时候应该是负无穷，但是曲线里面值 = 0）：<br>
$$<br>
H(p_1) = -p_1 \log_2(p_1) - (1 - p_1) \log_2(1 - p_1)<br>
$$</p>
<h4 id="信息增益">信息增益</h4>
<p>应用熵判断纯度：如下例，我们计算信息增益 = 根节点熵-左右两个节点的熵的加权平均值。</p>
<p><img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506042305567.png" alt=" "></p>
<p>下图是更为通用的公式。p 表示这个节点里面 true 占比（是猫），w 表示这个根分支分出去的动物数量占根节点总动物数量比。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506042307152.png" alt=" " style="zoom: 33%;" />
<p>拆分过程：</p>
<ol>
<li>汇总根节点数据。</li>
<li>计算不同拆分特征的信息增益，选择最大的一项进行拆分。</li>
<li>依次对左右子树继续拆分，直到满足我们之前说的停止拆分的条件为止（递归）。</li>
</ol>
<blockquote>
<p>[!NOTE]</p>
<p>树的深度效果类似于多项式的度数。深度越深对训练数据拟合性越好，但是可能出现过拟合问题。</p>
</blockquote>
<h4 id="离散特征划分：独热编码">离散特征划分：独热编码</h4>
<p>如果一个特征可以取出 k 个值，我们可以用独热编码用 k 位二进制位表示。</p>
<p>比如猫耳朵有尖耳圆耳折耳，这三个特征可以分别记作 [1,0,0] [0,1,0] [0,0,1]</p>
<p>不过显而易见独热编码不适合处理可以取任意值的情况，比如：尾巴长度，3cm~10.0cm（我乱说的）。</p>
<h4 id="连续特征划分：阈值">连续特征划分：阈值</h4>
<p>比如猫的体重，有 10 个值，我们从大到小排列，按 10 个点的中点选择 9 个阈值进行划分尝试，并计算信息增益。如果最大的信息增益还不错，就选择这个阈值作为一个决策树的节点划分依据。</p>
<h4 id="决策树回归问题">决策树回归问题</h4>
<p>比如我们要利用决策树，根据猫咪特征，预测猫咪体重。输出值是一个连续数值。</p>
<p>仍然是类似信息增益的算法，不过要借助方差判断划分的两个子树的数据的离散性，选择结果最大的。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506042327045.png" alt=" " style="zoom:80%;" />
<h4 id="决策树集合：投票">决策树集合：投票</h4>
<p>决策树模型其实对数据非常敏感。可能改一个数据，同样算法生成的决策树就完全不同。</p>
<p>所以比较保险的方法是我们构建多个决策树，对于一个预测问题多个树都预测并给出自身的预测结果，最后投票决定。</p>
<h5 id="替换采样">替换采样</h5>
<p>如何构建决策树集合？</p>
<p>假设有 m 条数据，我们现在随机抽取 m 次获取 m 条数据组成训练集（放回，也就意味着同一条数据可能被抽到多次，但是没关系），然后用这个训练集进行训练得到一个树。然后再抽再训练，继续循环。重复 B 次建立 B 棵树。</p>
<blockquote>
<p>[!WARNING]</p>
<p>一般 B 选 100 左右，再大未必会获得更好的结果。</p>
</blockquote>
<h5 id="随机森林">随机森林</h5>
<p>进一步提升随机性的方法是：在每个树的节点分类选择的时候，假设有 n 个特征可以选择，我们随机挑选 k &lt; n 个特征，规定该节点只能从这些特征里面选分类方法。k 一般选根号 n。这样可以进一步探索随机森林的更多随机性，防止数据的细微变化对预测正确率的影响。</p>
<h5 id="XGBOOST-训练方法">XGBOOST 训练方法</h5>
<p>和之前随机森林差不多，不过在选择 m 条数据组成训练集时，会更容易选择那些之前预测表现不那么好的数据。就像练习钢琴曲，总是练整首歌不如先优先练好不熟悉的部分。</p>
<p>此外，该算法也有很好的节点划分，停止划分，正交化等配置。在 kaggle 比赛中比较突出。</p>
<h4 id="什么时候用决策树？">什么时候用决策树？</h4>
<p>决策树的特点：</p>
<ul>
<li>适合结构化数据（如表格）；</li>
<li>不适用于非结构化数据（图片，音频）；</li>
<li>小决策树的可解释性强；</li>
<li>训练快。</li>
</ul>
<p>神经网络：</p>
<ul>
<li>适用于所有结构数据；</li>
<li>训练慢；</li>
<li>可迁移性好；</li>
<li>多模型协同工作表现好。</li>
</ul>
<h2 id="无监督学习-2">无监督学习</h2>
<p>没有标注的输出数据，只有输入数据，比如对现有数据进行聚类，让算法玩游戏，图片压缩，异常检测等。</p>
<h3 id="聚类">聚类</h3>
<p>比如下面这些无标注点，将他们分为几类。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506051135172.png" alt=" " style="zoom: 50%;" />
<h4 id="K-means">K-means</h4>
<ol>
<li>（假设要分为两个聚类）随机选择两个点作为质心。</li>
<li>计算其他点到这两个点的距离哪个更近，从而分为两类。</li>
<li>再找到这两个类的质心（所有点位置求平均值）。</li>
<li>再重新按所有点到两个新质心的距离分两类。</li>
<li>重复上述步骤直到质心位置不再发生变化时，说明收敛。</li>
</ol>
<p>换言之，就是让代价函数值最小：<br>
$$<br>
J(c^{(1)}, \ldots, c^{(m)}, \mu_1, \ldots, \mu_K)<br>
= \frac{1}{m} \sum_{i = 1}^{m} \left| x^{(i)} - \mu_{c^{(i)}} \right|^2<br>
$$<br>
k：目标要分为多少个聚类。</p>
<p>c_k：第几个聚类。</p>
<p>μ_k：质心。</p>
<p>$\mu ^{(i)}_c$：xi 点被分配到的质心。</p>
<p>这个函数被称作：失真函数（Distortion）。</p>
<h4 id="局部最小值问题">局部最小值问题</h4>
<p>虽然根据失真算法，我们每次更新质心都会让失真函数值下降，但是可能会达到局部极小值而并非最小值，如下图三种情况都收敛。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506051155655.png" alt=" " style="zoom: 33%;" />
<p>这和我们最开始随机选训练数据作为质心起始点有关，所以一般来说训练需要随机选多轮质心然后训练得到收敛的聚类模型，再比较选出失真函数值最小的最优解。</p>
<h4 id="K-？">K =？</h4>
<p>我们要将训练数据分为多少个聚类更为合适？</p>
<p>从训练角度上来说：选择 elbow 点，k 再增加对降低损失作用不大。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506051203592.png"  style="zoom:50%;" />
<p>同时我们也需要考虑实际解决方案，比如 T 恤分成 3 类和 5 类，随之带来的运输成本问题，客户满意度等都会影响营收。公司需要分别尝试两种解决方案并分析其带来的经济效益进行决定。</p>
<h3 id="异常检测">异常检测</h3>
<p>比如我们学习统计了 m 个训练数据（飞机发动机的热量和振动量，并且这些训练数据都是正常工作的飞机发动机，甚至可能表现更好一些），然后用一台飞机发动机的参数和这个模型分布进行对比，如果偏差比较大可能就说明这个发动机比较容易发生故障。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506051339018.png" alt=" " style="zoom:50%;" />
<h4 id="高斯分布">高斯分布</h4>
<p>异常检测的概率计算基于高斯分布。</p>
<ol>
<li>统计每个特征的期望 方差 获取高斯分布概率；</li>
<li>对于一个需要预测的数据，累乘其所有特征的高斯分布概率后判断是否大于阈值 ε，如果大于说明正常，小于说明异常。</li>
</ol>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506051351101.png" alt=" " style="zoom: 33%;" />
<h4 id="实时评估">实时评估</h4>
<p>一种可以在训练的时候评估学习算法的好坏并实时修改参数（如 ε）的方法。</p>
<p>可以借助少量有标签的异常数据构建交叉验证和测试集。比如 10000 台发动机，其中 20 个是异常的，一种数据集划分方式是 6000 个正常发动机作为训练集，2000 正常+10 异常发动机作为交叉验证集，2000 正常+10 异常发动机作为测试集。如果异常样本太少就放弃交叉验证集，异常样本全放到测试集。</p>
<h4 id="什么时候用监督学习，什么时候用异常检测？">什么时候用监督学习，什么时候用异常检测？</h4>
<p>监督学习场景：</p>
<ul>
<li>正例负例都够多。</li>
<li>未来要处理的正例和训练集正例相似。</li>
<li>我们更关心已知种类的识别，而并非未知种类的检测。</li>
</ul>
<p>如垃圾分类检测（垃圾邮件类型固定），天气预测（天气种类固定且容易判断），疾病诊断，单一异常检测（比如：该器件是否过度拉伸）。</p>
<p>异常检测场景：</p>
<ul>
<li>正例极少。</li>
<li>异常种类多样。</li>
<li>未来异常种类和现有历史数据可能完全不同。</li>
</ul>
<p>如数据中心黑客入侵安全监测，飞机引擎检测，金融欺诈检测。</p>
<h4 id="特征选择">特征选择</h4>
<p>在异常检测问题中，特征选择比监督学习中更为重要。因为监督学习再怎么说是有标注输出数据的，所以无关特征可能被弱化，但是异常检测是肯定会根据我们选择的特征项进行建模。</p>
<p>特征预处理：对特征数据预处理使其尽可能接近高斯分布。</p>
<p>对于错误分析案例，人为对误检案例进行分析，并寻找能检测这些漏检案例的新特征。</p>
<h3 id="协同过滤">协同过滤</h3>
<p>推荐系统常用。比如基于用户对电影的评分，将用户划分为某一类用户群体，并且给用户推荐该类群体喜欢的作品（基于用户）；或者基于该用户以往看过的电影的打分，预测一部该用户没看过的电影该用户会打多少分，从而推荐给用户（基于商品）。</p>
<h4 id="已知特征求模型">已知特征求模型</h4>
<p>比如如下案例，我们想根据每个用户过往给看过的电影打分，预测其会给没看过的电影打多少分。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506060049502.png" alt=" " style="zoom:50%;" />
<p>n_u：用户数量 = 4</p>
<p>n_m：电影数量 = 5</p>
<p>n：特征数量 = 2（一部电影百分之多少是浪漫，百分之多少是动作）</p>
<p>r(i, j) 表示用户 j 是否给电影 i 打过分，有 1 无 0.</p>
<p>y(i, j) 是用户 j 给电影 i 打多少分。</p>
<p>$x^{(i)}$ 表示第 i 部电影的特征列向量，比如第一个就是 [0.9, 0]</p>
<p>对于每个用户我们会生成一个预测模型，比如用户 1 生成的模型是 $w^{(1)}x^{(i)}+b^{(1)}$</p>
<p>代价函数公式：<br>
$$<br>
\min_{w^{(j)}, b^{(j)}} J(w^{(j)}, b^{(j)}) =<br>
\frac{1}{2m^{(j)}} \sum_{i: r(i, j)= 1} \left( w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i, j)} \right)^2 + \frac{lambda}{2m^{i}}\sum^{n}_{k = 1}(w_k^{(j)})^2<br>
$$</p>
<p>$$<br>
J\left(w^{(1)}, \ldots, w^{(n_u)}, b^{(1)}, \ldots, b^{(n_u)}\right) =<br>
\frac{1}{2} \sum_{j = 1}^{n_u} \sum_{i: r(i, j)= 1} \left( w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i, j)} \right)^2<br>
+\frac{\lambda}{2} \sum_{j = 1}^{n_u} \sum_{k = 1}^{n} \left(w_k^{(j)}\right)^2<br>
$$</p>
<h4 id="已知模型求特征值">已知模型求特征值</h4>
<p>假设我们现在知道 wb 但是不知道特征值，可以利用如下代价函数反向计算：<br>
$$<br>
J(x^{(i)}=\frac{1}{2} \sum_{j: r(i, j)= 1} \left( w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i, j)} \right)^2<br>
+\frac{\lambda}{2} \sum_{k = 1}^{n} \left( x_k^{(i)} \right)^2<br>
$$</p>
<p>$$<br>
J(x^{(1)}, x^{(2)}, \ldots, x^{(n_m)}) =<br>
\frac{1}{2} \sum_{i = 1}^{n_m} \sum_{j: r(i, j)= 1}<br>
\left( w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i, j)} \right)^2<br>
+\frac{\lambda}{2} \sum_{i = 1}^{n_m} \sum_{k = 1}^{n} \left( x_k^{(i)} \right)^2<br>
$$</p>
<p>公式和之前基本一样 就是把正交化部分由 x 替换了 w 参数。</p>
<p>这个应用场景比如根据推荐系统我们可以推测出这部电影的特征成分，我爱看动作片，算法给我推荐这个电影说明动作成分高。</p>
<h4 id="二元标签问题">二元标签问题</h4>
<p>比如 1 表示用户接触这部电影后选择看了，0 表示其接触这部电影后选择不看，预测一部用户没接触过的电影他会不会看。或者推荐给用户的商品会不会买，会不会点击，视频会不会看 30s 以上。</p>
<p>这些就是把协同过滤回归问题改成二元标签分类问题了。使用 sigmoid 函数，逻辑代价函数等。<br>
$$<br>
f_{w, b, x}(x) = g(w^{(j)} \cdot x^{(i)} + b^{(j)})\\<br>
L(f_{w, b, x}(x), y^{(i, j)}) =<br>
-y^{(i, j)} \log(f_{w, b, x}(x)) - (1 - y^{(i, j)}) \log(1 - f_{w, b, x}(x))\\<br>
J(w, b, x) = \sum_{(i, j): r(i, j)= 1} L(f_{w, b, x}(x), y^{(i, j)})<br>
$$</p>
<h4 id="均值归一化预测新用户问题">均值归一化预测新用户问题</h4>
<p>对于根据用户喜好预测一部电影用户会打多少分的问题，没法推广到新用户，因为我们不知道新用户的喜好模型（w b = 0），只能预测出 0 分的结果。</p>
<p>一种解决方法是，我们根据已有用户的喜好求出每部电影的评分均值，然后每个用户的评分都减去均值在进行训练（正态化 Normalization）。</p>
<p>预测模型变为 y = wx+b+μ. 这样对于新用户对一部电影的打分就不是 0 分，而是这部电影的均值评分了。</p>
<h4 id="协同过滤的-Tensorflow-实现">协同过滤的 Tensorflow 实现</h4>
<p>和之前有一些区别，比如参数不是层的权重而是嵌入矩阵的值（X w b）；代价函数要自己写而不能 forward 自动使用（因为矩阵是稀疏矩阵 并不是所有数值都有的，而且预测是线性并非神经网络层）。</p>
<h4 id="找到和当前商品最相近的商品">找到和当前商品最相近的商品</h4>
<p>比如用户在浏览一部电影，这部电影的 action 特征是 0.1，romantic 特征是 0.9，推荐系统可能会推荐类似这个特征比例的电影在该电影的下面（猜你想看）。<br>
$$<br>
\sum_{l = 1}^{n} \left( x_l^{(k)} - x_l^{(i)} \right)^2<br>
$$</p>
<h4 id="协同过滤的局限">协同过滤的局限</h4>
<p>推荐算法可能会遇到一些“冷启动”问题（cold start）。比如，一部新发布的电影，可能还没有多少用户打分，我们不知道它怎么样没法推荐；以及对于一个没发表过几次评论的用户，我们也不知道其喜好。</p>
<p>我们可以使用一些辅助信息（side information）来解决此类问题，比如用户没发表过什么评论也可以根据其年龄性别地域等猜测其喜好；电影也可以根据其关键词，电影类型等进行推荐。</p>
<h3 id="基于内容的过滤">基于内容的过滤</h3>
<p>基于用户判断其喜好获得一个向量（比如：基于该用户的年龄，性别等内容判断其对浪漫电影的喜好程度为 0.9）乘以基于物品内容对物品成分的判断向量（比如：这部电影的浪漫成分为 0.3）两个向量相乘+b 预测该用户对该电影的喜好。</p>
<p>比如通过神经网络学习，获取用户喜好向量和电影成分向量：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061720414.png" alt=" " style="zoom: 33%;" />
$$
J = \sum_{(i, j):\ r(i, j) = 1} \left( \mathbf{v}_u^{(j)} \cdot \mathbf{v}_m^{(i)} - y^{(i, j)} \right)^2\ +  \ NN \  Regularization
$$
第 j 个 user 的特征向量是 $x^{(j)}\_u$，第 i 部电影的特征向量是 $z^{(i)}\_m$. 求两个用户喜好相似度或者两部电影相似度还是用特征向量距离平方的方法。
<h4 id="大型目录">大型目录</h4>
<p>但是商品数量可能过多（比如电影成千上万部），挨个放入模型判断用户预测满意度并不合适。这样可能可以获得更好的性能但是太浪费时间和计算资源</p>
<p>主要分为两步：</p>
<ul>
<li>检索（Retrieval）：比如先基于用户最近观看的 10 部电影找最相似的 10 部，然后基于用户最喜欢的三类电影找较为匹配的前十部电影；再找用户特定规则集中评分最高的电影（比如用户所在国家的电影排行榜），从中去掉重复和用户已观看的项目。</li>
<li>评分（Ranking）：再从这些电影中利用基于内容过滤推荐算法进行评分排行。</li>
</ul>
<h4 id="道德伦理问题">道德伦理问题</h4>
<p>要善用推荐算法，负面例子比如为了吸引流量引战行为，或者旅游公司形成压榨客户-&gt; 赚更多的钱-&gt; 投资广告来压榨更多的客户这样的循环，排行榜中加入很多广告，售卖用户喜好隐私等。</p>
<h3 id="PCA-主成分分析">PCA 主成分分析</h3>
<p>可以将大量特征降维为少数特征。</p>
<p>比如为了衡量汽车大小，我们需要汽车长度和宽度两维特征。不过根据汽车生产经验其实长宽一般是呈现一定比例的（比如，我是说比如 长度 = 2*宽度），那么这样我们就可以只保留一个特征宽度。</p>
<p>其实从图像角度上来说，宽度是 x 长度是 y 的话，长宽大致呈现 y = 2x 的直线，那么我们可以让 y = 2x 这条直线变成新的一维特征，所有宽高的二维点都垂直投影到这条直线上降维。</p>
<blockquote>
<p>[!WARNING]</p>
<p>PCA 比较容易和线性回归混淆。线性回归的 y 是标注输出数据，我们要根据 x 预测 y 并根据和真实值之间的差距迭代修改算法。而 PCA 是无监督学习，我们要找到一个维度让原数据二维坐标投影到上面的距离平方和最小。</p>
<p><img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061759977.png" alt=" "></p>
</blockquote>
<p>PCA 应用：比如数据可视化（人顶多能看懂 2-3 维度数据），以及数据压缩（不常用）。</p>
<p>sk-learn 可以将原始数据训练得到降维后的数据，还可以还原原数据（当然会丢失一部分信息）。</p>
<h2 id="强化学习">强化学习</h2>
<p>不直接给予模型标注输出结果 y，而是给予一个奖励函数指导每一步的收益。</p>
<p>比如我们期望吃豆人赢得游戏，但是吃豆人在一局游戏里面有上百步可以走，如果我们用吃豆人这一局的上百次决策作为输入，是否赢得游戏作为输出，不知道要训练到猴年马月。</p>
<p>奖励函数是某一步会有一点奖惩机制，比如吃到豆子+奖励，离鬼近了减奖励，被鬼吃掉奖励减为零等，多次训练后让吃豆人自己做出更偏向于获得高奖励的结果：赢得游戏。</p>
<p>除了玩游戏，其他应用还有控制机器人，金融股票交易，工厂设备自动优化等。</p>
<p>比如一个简单的强化学习案例：火星探测器。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061822480.png" alt=" " style="zoom: 67%;" />
<p>我们将 1-6 这些机器人可以到达的位置称作 state。机器人起始位于 state 4 的位置，每次可以选择向左走或者向右走的行动（action）。位置 1 和位置 6 各有两个科学家很感兴趣的地形，所以被赋予了很高的奖励值（Reward）鼓励探测器去探测。但是只能选择一个</p>
<p>假设探测器采取的行动是左左左，那么它就会最终获得 100 点奖励。假设采取的行动是右右，就会获取 40 点奖励（当然这里我们简化了代价函数，探测器能源并不是无尽的。后面会补充）。</p>
<p>每次行动我们用 (s, a, R(s), s’) 表示，s 是当前探测器所处状态，a 是探测器下一步采取的行动，R(s) 是当前探测器的奖励（4 位置，奖励为 0），s’ 是采取行动 a 后会到达的 s 新状态。</p>
<p>汇报（Return）则是从一个状态到一个状态的所有奖励值相加。</p>
<h3 id="折扣因子">折扣因子</h3>
<p>为了限制探测器尽快解决问题，我们引入折扣因子 γ（discount factor）。γ &lt; 1，让探测器随着时间推移能获得的奖励值逐渐下降。</p>
<p>比如探测器采取往左走 3 步的行动，获得的回报是 0+0.9*0+0.9*0.9*0+0.9*0.9*0.9*100.（初始位置 reward 不用乘折扣因子）</p>
<p>这样可以减小探测器采取“左右左右左右”这种无限耗时间的策略的情况发生，因为越早到达 1 地点获得的回报越高。不然探测器会不着急到达 1 和 6 地点，反正早到晚到回报一样。</p>
<p>引入折扣因子后探测器采取的最佳行动是左左左到达 state1，这样比到达 6 损失少。</p>
<h3 id="策略">策略</h3>
<p>策略（policy）用 π 表示，代表在某一状态下智能体（Agent）根据其策略会采取什么行动。$\pi(s)=a$</p>
<h3 id="马尔科夫链">马尔科夫链</h3>
<p>Agent 采取 action a 后，根据 environment 读取到当前 state s 以及获得的 reward r。</p>
<h3 id="Q-函数">Q 函数</h3>
<p>$$<br>
Q^{\pi}(s, a)= E_{\pi}[G_t | s_t = s, a_t = a]<br>
$$</p>
<p>表示在状态 s 采取动作 a，并遵循策略 π，未来能获得的期望回报。</p>
<blockquote>
<p>[!Note]</p>
<p>Return 是采样数据，相当于根据过往经验我们知道这个 Agent 现在往左走能获得多少回报。Q 函数则是我们要预测的模型。</p>
</blockquote>
<p>如下例，假设探测器当前位于位置 2，下一步 action 往右走，之后采取最优策略，γ = 0.5. Q = 0（当前奖励）+0*0.5（下一步奖励）+0*0.5*0.5 + 100 * 0.5 * 0.5 * 0.5（最优策略就是一直往左）。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061851470.png" alt=" " style="zoom:50%;" />
<p>根据 Q 函数我们可以标注出处于每个位置，采取不同 action 的不同 Q 值：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061856963.png" alt="image-20250606185634716" style="zoom: 80%;" />
<p>这样就比较显而易见了，在 1-4 位置的 action 最好是往左，5-6 是往右（1,6 已经结束游戏了其实）。</p>
<blockquote>
<p>[!NOTE]</p>
<p>Q* 表示最优策略。</p>
</blockquote>
<h3 id="比尔曼方程">比尔曼方程</h3>
<p>用于计算 Q 函数。<br>
$$<br>
Q(s, a) = R(s) + \gamma \max_{a’} Q(s’, a’)<br>
$$</p>
<h3 id="随机环境">随机环境</h3>
<p>由于一些随机环境因素影响，智能体的行动并非百分百能执行的。比如探测器往左走，结果左边的地面打滑导致方向偏移等情况。所以需要引入随机因素。</p>
<p>比如探测器 90%的概率往指定的方向走，10%的概率会往反方向走。所以采取一个行动的 Q 就变成求采取这个行动后的期望 Q，即 0.9*往左走的 Q+0.1*往右走的 Q。</p>
<p>相应的比尔曼方程计算公式变为：<br>
$$<br>
Q(s, a) = R(s) + \gamma , \mathbb{E}\left[ \max_{a’} Q(s’, a’) \right]<br>
$$</p>
<h3 id="连续状态空间问题">连续状态空间问题</h3>
<p>上面的只是一个非常简单的离散状态空间问题。现实生活中的问题大多数是连续的。比如一辆汽车的状态空间可能包括 $[x, y, \theta, \dot{x}, \dot{y}, \dot{\theta}]$ ，x y 是位置坐标，θ 是当前角度，求导是水平垂直方向上的速度和角速度。三维空间如无人机会更加复杂，还可能引入加速度等。</p>
<h4 id="小游戏：Lunar-Lander">小游戏：Lunar Lander</h4>
<p>调整月球登陆器的左，中，右推进器（或不采取行动）让其能平稳落在黄旗中间。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061934300.png" alt=" " style="zoom:50%;" />
<p>状态空间：$[x, y, \dot{x}, \dot{y}, \theta,  \dot{\theta}, l, r]$，l r 是二进制表示左腿右腿是否着地。</p>
<p>如下是一些奖励函数，是否落在登陆区域；距离登陆区域的左右偏移量；坠毁；软着陆；腿着陆；以及避免消耗太多燃料。折扣因子 0.985.</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061937951.png" alt="image-20250606193751573" style="zoom: 50%;" />
<p>我们计划建立一个神经网络 计算 Q 函数：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061940997.png" alt="image-20250606194018738" style="zoom:50%;" />
<p>然后在每个时刻比较 Q(s, nothing)  Q(s, left)  Q(s, main)  Q(s, right) 哪个奖励最高。</p>
<h4 id="DQN">DQN</h4>
<p>如下循环迭代更新 Q 函数。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061945274.png"  style="zoom: 50%;" />
<p>改进的训练方式如下，同时训练4个 Q 函数。</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506061946353.png" alt=" " style="zoom: 33%;" />
<h4 id="ε-贪心算法">ε-贪心算法</h4>
<p>在训练过程中引入了我们之前说的随机环境问题。比如ε=0.05，那么推进器右95%的概率使用我们制定的 action，5%概率使用其他随机行为。</p>
<p>比如主推进器，可能我们的策略并不会经常性尝试使用它（作用不明显），但是引入一些随机尝试后也许就发现主推进器在某些情况下会取得不错的效果，那么 Q 函数就会更新这一部分让其获得更大的权重。95%的指定行动部分就是贪心部分，而5%的部分是探索部分。</p>
<h3 id="小批量训练">小批量训练</h3>
<p>之前的训练方法基本都是探索所有点求代价函数然后再修正。比如线性回归，我们定义了一个模型，然后求所有数据点到这个线的距离的平方和求和作为代价，再调整模型。</p>
<p>但是如果数据量非常大，这个方法就并不适用。我们可以先取小批量（mini batch）部分点，比如随机选择1000个点进行训练并更新参数，然后再取1000个。</p>
<p>上面这个连续状态空间问题中的10000个缓存也可以用同样的方法优化一下，我们可能每次只取1000个数据进行缓存然后迭代更新。</p>
<p>这样的缺点在于更新可能并不准确，并非一定是沿着梯度下降的方向进行的，如下图：</p>
<img src="https://raw.githubusercontent.com/Jingqing3948/FigureBed/main/mdImages/202506062011763.png" alt=" " style="zoom: 67%;" />
<p>所以，我们也可以不完全更新参数权重。比如一次小批量后得到的新参数是 w_new 和 b_new，那我们就更新参数 w = 0.99w+0.01 w_new, b = 0.99b+0.01 b_new 这样避免突变带来一些大影响。这种更新策略叫做软更新（soft update）</p>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Contact Me
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://jingqing3948.github.io/2025/05/31/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
    
      <a href="/2025/05/29/%E9%A2%98%E7%9B%AE%E7%BB%83%E4%B9%A0%EF%BC%9A%E4%BB%A3%E7%A0%81%E9%9A%8F%E6%83%B3%E5%BD%95%E8%A7%A3%E9%A2%98%E6%80%9D%E8%B7%AF%E5%88%86%E6%9E%90/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">题目练习：代码随想录解题思路分析</div>
      </a>
    
  </nav>

  
   
  <div class="gitalk" id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.staticfile.org/gitalk/1.7.2/gitalk.min.css">

  <script>
    window.addEventListener('DOMContentLoaded', function () {
      const gitalkScript = document.createElement('script');
      gitalkScript.src = 'https://cdn.staticfile.org/gitalk/1.7.2/gitalk.min.js';
      gitalkScript.onload = function () {
        const md5Script = document.createElement('script');
        md5Script.src = 'https://cdn.staticfile.org/blueimp-md5/2.19.0/js/md5.min.js';
        md5Script.onload = function () {
          const gitalk = new Gitalk({
            clientID: 'Ov23li1TkQEYXFu51qLu',
            clientSecret: '0c4ca8b5adfcc2aedd741ba78ca96fcbf2c4f5b5',
            repo: 'Jingqing3948.github.io',
            owner: 'Jingqing3948',
            admin: ['Jingqing3948'],
            id: md5(location.pathname),
            distractionFreeMode: true,
            pagerDirection: 'last'
          });
          gitalk.render('gitalk-container');
        };
        document.body.appendChild(md5Script);
      };
      document.body.appendChild(gitalkScript);
    });
  </script>

  
   
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2022-2025
        <i class="ri-heart-fill heart_icon"></i> Jingqing3948
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="灰海宽松的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/gallery">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="https://blog.csdn.net/jtwqwq?spm=1000.2115.3001.5343">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="https://jingqing3948.github.io/Tianweijiang.github.io/">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>星光不问，梦终有回</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/LinkedIn.jpg">
      <span class="reward-type">LinkedIn</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/WXOfficalAccount.jpg">
      <span class="reward-type">公众号</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      showProcessingMessages: false,
      messageStyle: 'none',
      skipStartupTypeset: false
    };
  </script>
  <script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full"></script>


<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script>

<script src="/js/clickBoom1.js"></script>
 
<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>